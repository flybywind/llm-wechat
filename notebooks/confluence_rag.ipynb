{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from llmagent.secret import AK_SK\n",
    "keys = AK_SK('../llmagent/secret/keystore/qianfan.keys')\n",
    "os.environ[\"QIANFAN_ACCESS_KEY\"] = keys.get_ak()\n",
    "os.environ[\"QIANFAN_SECRET_KEY\"] = keys.get_sk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING][2024-10-21 09:25:06.918] redis_rate_limiter.py:21 [t:8703163072]: no redis installed, RedisRateLimiter unavailable\n"
     ]
    }
   ],
   "source": [
    "from llmagent.llmapi import QianfanLLM, QianfanEmbedding, model_spec\n",
    "\n",
    "llm = QianfanLLM(model_spec=model_spec.Speed128K, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlassian import Confluence\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers.transform import BaseTransformOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ConfluenceLoader\n",
    "from langchain.schema import BaseOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\"qianfan\", embedding_function=QianfanEmbedding(), persist_directory=\"../vectorstore/fintopia.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "505"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一次初始化时运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding documents 0 to 16\n",
      "Adding documents 16 to 32\n",
      "Adding documents 32 to 48\n",
      "Adding documents 48 to 64\n",
      "Adding documents 64 to 80\n",
      "Adding documents 80 to 96\n",
      "Adding documents 96 to 112\n",
      "Adding documents 112 to 128\n",
      "Adding documents 128 to 144\n",
      "Adding documents 144 to 160\n",
      "Adding documents 160 to 176\n",
      "Adding documents 176 to 192\n",
      "Adding documents 192 to 208\n",
      "Adding documents 208 to 224\n",
      "Adding documents 224 to 240\n",
      "Adding documents 240 to 256\n",
      "Adding documents 256 to 272\n",
      "Adding documents 272 to 288\n",
      "Adding documents 288 to 304\n",
      "Adding documents 304 to 320\n",
      "Adding documents 320 to 336\n",
      "Adding documents 336 to 352\n",
      "Adding documents 352 to 368\n",
      "Adding documents 368 to 384\n",
      "Adding documents 384 to 400\n",
      "Adding documents 400 to 416\n",
      "Adding documents 416 to 432\n",
      "Adding documents 432 to 448\n",
      "Adding documents 448 to 464\n",
      "Adding documents 464 to 480\n",
      "Adding documents 480 to 496\n",
      "Adding documents 496 to 512\n"
     ]
    }
   ],
   "source": [
    "def get_children_pages_recursively(client, page_id: str):\n",
    "    child_pages = client.get_page_child_by_type(page_id)\n",
    "    for page in child_pages:\n",
    "        yield page\n",
    "        if \"id\" in page:\n",
    "            yield from get_children_pages_recursively(client, page[\"id\"])\n",
    "\n",
    "from getpass import getpass\n",
    "wiki_psw = getpass(\"wiki:\")\n",
    "# get all pages under some page\n",
    "confluence = Confluence(\n",
    "    url='https://wiki.fintopia.tech/',\n",
    "    username='tongxinwen@fintopia.tech',\n",
    "    password=wiki_psw)\n",
    "child_pages = [p for p in get_children_pages_recursively(confluence, \"74290517\")]\n",
    "\n",
    "loader = ConfluenceLoader(\n",
    "    url=\"https://wiki.fintopia.tech/\",\n",
    "    page_ids=[p[\"id\"] for p in child_pages],\n",
    "    username=\"tongxinwen@fintopia.tech\",\n",
    "    api_key=wiki_psw,\n",
    "    limit=10,\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "batch_size = 16\n",
    "doc_ids = []\n",
    "for i in range(0, len(splits), batch_size):\n",
    "    print(f\"Adding documents {i} to {i+batch_size}\")\n",
    "    texts = [doc.page_content for doc in splits[i:i+batch_size]]\n",
    "    metadatas = [doc.metadata for doc in splits[i:i+batch_size]]\n",
    "    doc_ids += vectorstore.add_texts(texts, metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': '95160416', 'source': 'https://wiki.fintopia.tech/display/riskDev/2024H2', 'title': '2024H2', 'when': '2024-07-09T17:39:33.000+08:00'}, page_content='200ms KR3 模型推理框架国内外统一 【P2】 KR4 模型cache表优化，提高插入和查询效率，同时降低存储压力【P2】 建立HBase + Hive + PolarDB 3位一体的多级缓存 查询方面通过PolarDB + HBase提供高效查询，存储方面通过Hive提供长期存储功能，方面回溯各种问题 建立各级存储模块的过期策略，在PolarDB中存储原来的30%的数据，HBase存储未过期model的所有记录，hive存储最近3年的所有记录 O5 海外平台搭建 【P0】 以菲律宾、印尼为主 目标，训练全部迁移到ailab，推理迁移到新框架。'),\n",
       " Document(metadata={'id': '95160416', 'source': 'https://wiki.fintopia.tech/display/riskDev/2024H2', 'title': '2024H2', 'when': '2024-07-09T17:39:33.000+08:00'}, page_content='200ms KR3 模型推理框架国内外统一 【P2】 KR4 模型cache表优化，提高插入和查询效率，同时降低存储压力【P2】 建立HBase + Hive + PolarDB 3位一体的多级缓存 查询方面通过PolarDB + HBase提供高效查询，存储方面通过Hive提供长期存储功能，方面回溯各种问题 建立各级存储模块的过期策略，在PolarDB中存储原来的30%的数据，HBase存储未过期model的所有记录，hive存储最近3年的所有记录 O5 海外平台搭建 【P0】 以菲律宾、印尼为主 目标，训练全部迁移到ailab，推理迁移到新框架。'),\n",
       " Document(metadata={'id': '91856753', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=91856753', 'title': '7. 在AiLab中连接hive', 'when': '2024-09-03T14:48:03.000+08:00'}, page_content='因为权限管理和LDAP一样，所以用户如果想要申请某个表的权限，需要去数仓申请 https://dw-lighthouse.fintopia.tech/tools/hive-permission 另外，如果不想把自己的密码保存在ipynb中，可以使用getpass函数，如下，这样就保存在passwd这个变量里了 ！！注意 ！！ hive.read_sql 这句话会把整个表下载到本地。强烈建议先在SQL中对数据进行过滤，采样或者group。 比如如果想知道某个表的行数，我们应该这样做： py 而不是这样：否则很可能把AiLab打挂 py 向hive中导入表 py \",'),\n",
       " Document(metadata={'id': '91856753', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=91856753', 'title': '7. 在AiLab中连接hive', 'when': '2024-09-03T14:48:03.000+08:00'}, page_content='因为权限管理和LDAP一样，所以用户如果想要申请某个表的权限，需要去数仓申请 https://dw-lighthouse.fintopia.tech/tools/hive-permission 另外，如果不想把自己的密码保存在ipynb中，可以使用getpass函数，如下，这样就保存在passwd这个变量里了 ！！注意 ！！ hive.read_sql 这句话会把整个表下载到本地。强烈建议先在SQL中对数据进行过滤，采样或者group。 比如如果想知道某个表的行数，我们应该这样做： py 而不是这样：否则很可能把AiLab打挂 py 向hive中导入表 py \",')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorstore.search(\"数据库\", search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Rag chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# 感觉需要精简一下问题模版。现在llm中会自动记录之前的问答历史，但是如果增加了context，那么每次都需要api处理多个相同的context，感觉非常浪费\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", \"你是一个解答用户问题的assistant，可以根据从语料库中召回的context文本回答问题。请尽量保证回答的内容都可以在context中找到根据，并务必保留 source 后面的url链接。\\n以下是context文本：{context}，\\n问题是：{question}\"),\n",
    "            (\"assistant\", \"根据context文本，我认为答案是：\"),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': '91856753', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=91856753', 'title': '7. 在AiLab中连接hive', 'when': '2024-09-03T14:48:03.000+08:00'}, page_content='因为权限管理和LDAP一样，所以用户如果想要申请某个表的权限，需要去数仓申请 https://dw-lighthouse.fintopia.tech/tools/hive-permission 另外，如果不想把自己的密码保存在ipynb中，可以使用getpass函数，如下，这样就保存在passwd这个变量里了 ！！注意 ！！ hive.read_sql 这句话会把整个表下载到本地。强烈建议先在SQL中对数据进行过滤，采样或者group。 比如如果想知道某个表的行数，我们应该这样做： py 而不是这样：否则很可能把AiLab打挂 py 向hive中导入表 py \",'),\n",
       " Document(metadata={'id': '97182399', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=97182399', 'title': '模型迁移ModelServe', 'when': '2024-09-09T19:29:40.000+08:00'}, page_content='优缺点:直接读库开发简单,但是需要频繁扫model_result大表(百亿级),可能会有性能问题. 方案一:直接通过hivesql,通过hive直接对比增量数据 优缺点:通过hive的资源去做对比避免影响线上mysql性能,但是需要注意大批量的对比sql可能会超时. 5.一键切换模型服务到新 通过mirror服务核对后,确认模型迁移后无diff,需要提供一键转换功能,将模型的部署方式转换为ModelServe部署,主要更新MODEL_DATA中的deployType为3.然后清楚掉模型的缓存,让新的预测重新从数据库加载进行'),\n",
       " Document(metadata={'id': '97182399', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=97182399', 'title': '模型迁移ModelServe', 'when': '2024-09-09T19:29:40.000+08:00'}, page_content='模型多框架同时注册并开启陪跑后,model_result_mirror表会存模型转写新框架的预测结果,通过和model_result表线上结果做核对,来确认转写的模型是否可迁移. 新建job,定时核对mirror预测结果和线上结果diff 方案一:增量扫描model_result_mirror表数据,然后根据结果去查询model_result表直接进行核对(注意分批查询,使用trace和model的联合索引) 优缺点:直接读库开发简单,但是需要频繁扫model_result大表(百亿级),可能会有性能问题. 方案一:直接通过hivesql,通过hive直接对比增量数据'),\n",
       " Document(metadata={'id': '99949500', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=99949500', 'title': '2024-07. DAG流水线支持', 'when': '2024-08-09T13:58:10.000+08:00'}, page_content='}]]> mysql 表定义 sql 关键函数介绍 pipeline相关 创建pipeline PipelineDataVO createNew (PipelineDataVO pipelineDataVO) 用于第一次创建一个新流图。会在pipeline表中创建一个空图，并返回其pipeline id。 编辑pipeline boolean beforeEdit (long pplId， String uerId)\\xa0// 返回是否在草图表中已经有了 PipelineDataVO startEdit (long pplId, String userId, boolean'),\n",
       " Document(metadata={'id': '91856753', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=91856753', 'title': '7. 在AiLab中连接hive', 'when': '2024-09-03T14:48:03.000+08:00'}, page_content='执行SQL 目前我们已经把AILab的网络和hive的主机打通，用户可以直接在AiLab中访问hive，并转化成pandas的DataFrame进行使用。 具体使用方法如下： 使用dsw-lgb-xgb-py38:v1.12及以上版本的镜像 创建ailab。具体见 打开jupyter，demo如下 py 运行结果如下： 因为权限管理和LDAP一样，所以用户如果想要申请某个表的权限，需要去数仓申请 https://dw-lighthouse.fintopia.tech/tools/hive-permission'),\n",
       " Document(metadata={'id': '78279979', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=78279979', 'title': '数据探查任务集成到etl pipeline', 'when': '2024-07-04T14:48:07.000+08:00'}, page_content='RUNNING\\xa0→ COMPLETED\\xa0→ HIVE_FINISHED → DW_PUSHING 3 方案提议 3.1 数据探查服务 从第二部分的描述可知，最终etl job都是通过automl scheduler的一个离线job进行状态检查和流转的。这一步逻辑是统一的。所以可以把我们的数据探查任务分别加入HiveQueryExportFeatureStatusJob和FeatureExportTaskScanJob这2个job，当任务状态变成HIVE_FINISHED后，调用数仓的一个接口，比如我们可以创建一个单独的接口')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.search(\"数据库\", search_type=\"similarity\", k=6)\n",
    "# vectorstore.search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import Field\n",
    "\n",
    "class VectorStoreRetrieverMoreHistory(BaseRetriever):\n",
    "    vs: VectorStore\n",
    "    llm: LLM\n",
    "    topk: int = Field(gt=0, default=5, description=\"Number of documents to retrieve\")\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, chat_str: str, *, run_manager):\n",
    "        ai_ans = [c['content'] for c in self.llm.conversation_history if c['role'] == 'assistant']\n",
    "        user_quest = self.llm.user_question_history\n",
    "        qa_pairs = []\n",
    "        for q, a in zip(user_quest, ai_ans):\n",
    "            qa_pairs += [q, a]\n",
    "            \n",
    "        if chat_str.startswith(\".. \"):\n",
    "        # append the last 2 conversation \n",
    "            query = \"\\n\".join(qa_pairs[-2:] + [chat_str[3:]])\n",
    "            self.llm.add_user_question(chat_str[3:])\n",
    "        elif chat_str.startswith(\".... \"):\n",
    "            # append all conversation history\n",
    "            query = \"\\n\".join(qa_pairs + [chat_str[5:]])\n",
    "            self.llm.add_user_question(chat_str[5:])\n",
    "        else:\n",
    "            self.llm.clear_history()\n",
    "            query = chat_str\n",
    "            self.llm.add_user_question(query)\n",
    "        return self.vs.similarity_search(query, k=self.topk)\n",
    "\n",
    "retriever = VectorStoreRetrieverMoreHistory(vs=vectorstore, llm=llm, topk=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"{d.page_content}\\nsource: {d.metadata['source']}\" for d in docs) \n",
    "\n",
    "llm.clear_history()\n",
    "    \n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING][2024-10-15 23:56:25.169] base.py:916 [t:8703163072]: This key `disable_search` does not seem to be a parameter that the model `ERNIE-Speed-128K` will accept\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'公共资源池和私有资源池的主要区别在于资源获取方式和计费方式。\\n\\n公共资源池是阿里云的后付费资源池，用户可以根据需求申请使用，按照实际使用的小时数进行计费，不用则不计费。创建和启动实例的过程相对较慢。\\n\\n私有资源池则是用户预付费购买的资源池，创建和启动实例的速度较快。用户在使用时需要先付费购买资源，然后才能使用。相比于公共资源池，私有资源池的资源更加稳定，不会因资源紧张而导致任务提交失败。但是，如果多人同时使用私有资源池，可能会导致资源不足，需要排队等待或者进入初始化状态。因此，在选择资源池时需要根据实际需求进行选择。以上信息来源于context文本中的描述。'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"公共资源池和私有资源池的区别\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING][2024-10-15 23:56:41.136] base.py:916 [t:8703163072]: This key `disable_search` does not seem to be a parameter that the model `ERNIE-Speed-128K` will accept\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'分布式LightGBM模型训练可以通过以下步骤进行：\\n\\n1. 数据分片：将数据集分成多个分片文件，每个分片文件包含数据的一部分。分片的数量需要是并行处理节点的整数倍。\\n2. 分布式计算框架的搭建：利用Ray等分布式计算框架，构建多机的分布式计算环境。\\n3. 提交训练任务：通过Ray框架提交LightGBM的训练任务到集群上，开启分布式训练。\\n4. 数据加载和模型训练：平台会自动将训练集和测试集的数据平均分配到各个worker节点上，然后利用LightGBM官方提供的训练API进行模型训练。在分布式训练过程中，每个机器都会在自己本地数据集上进行训练，并通过某种投票算法选出最优的模型更新到最终的模型里。\\n\\n请注意，以上步骤需要根据具体情况进行调整和优化，以确保分布式训练的效果和效率。另外，分布式训练可能会带来一些精度损失，需要根据实际需求进行权衡和选择。如有更多疑问或需求，建议参考LightGBM官方文档或咨询相关领域的专家。'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"如何进行分布式LightGBM模型训练\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING][2024-10-15 23:57:00.876] base.py:916 [t:8703163072]: This key `disable_search` does not seem to be a parameter that the model `ERNIE-Speed-128K` will accept\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'在进行分布式训练时，需要具备以下前提条件：\\n\\n1. 数据集必须是分片好的，且分片个数需要是并行处理节点的整数倍。\\n2. 数据集必须是parquet格式。\\n3. 训练集和测试集的数据需要自动分配到各个处理节点上，以减少load data带来的开销和单节点的内存压力。\\n4. 需要准备训练参数，这些参数通过yaml文件定义。\\n5. 在使用某些服务或功能时，可能需要用户的label列、train和valid数据集，且这些数据集必须放在同一个父目录下。\\n6. 对于使用AiLab的api提交的情况，训练样本的parquet格式最好不要超过60G，否则可能会出现内存不足无法训练的情况。\\n\\n以上是根据context文本提供的信息整理的需要具备的前提条件，如果还有其他细节要求或特定环境的要求，请进一步确认。'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\".. 需要具备哪些前提条件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING][2024-10-15 23:57:21.156] base.py:916 [t:8703163072]: This key `disable_search` does not seem to be a parameter that the model `ERNIE-Speed-128K` will accept\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'参数建议的设置需要根据具体情况而定，对于可选参数，可以使用默认值；对于必选参数，则需要按照要求填写具体的参数值。\\n\\n在执行容器时，框架会把参数注入到命令行中，用户需要保证脚本可以接受这样的参数。对于参数的具体设置，可以参考相关官方文档或教程，如Papermill的官网教程。此外，根据实验结果和模型表现，可以对参数进行调整和优化，例如通过HPO自动寻参的方式，自动搜索最优参数。\\n\\n对于超参数调优，可以使用成熟的Search算法（如贝叶斯优化、GridSearch等）选择最优值。在设置参数时，需要根据具体的模型和任务来确定哪些参数需要调整，以及调整的范围和步长等。\\n\\n以上信息仅供参考，具体的参数设置建议还需要根据实际需求和实践经验进行调整。'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\".. 参数建议如何设置\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Human: 你是一个解答用户问题的assistant，可以根据从语料库中召回的context文本回答问题。请尽量保证回答的内容都可以在context中找到根据，并务必保留 source 后面的url链接。以下是context文本：当资源不够的时候会进入排队，实例会一直处于初始化状态。 解释一下这二者的区别， 固定资源池是我们自己买的一个小池子，这部分是预付费的，即钱已经花出去了，不用就浪费了。特点是：创建和启动实例快，但是如果用的人多了可能没资源。 公共资源池是后付费的，根据小时数计费，不用不计费，属于阿里云的大池子。特点是创建和启动实例比较慢。 建议大家先尝试创建or启动内存型资源池。没资源会提示。如果提示了，再去申请公共资源池 （2）选择弹性资源池时，配置与现状相同 注意，使用固定资源池时，镜像会优先选择在本地缓存的版本。所以第一次拉取镜像的时间会比较长，后面就快很多了。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=80187151\\n\\n）当用户提交任务时，我们会优先选择私有资源池的资源，如果私有资源池没有资源了，我们才选择公有资源池的CPU机器。之所以公有资源池不选择GPU机器，是因为GPU库存经常紧张，很容易导致任务提交失败。 PS，另外，我们也调研了一下单机多卡的模式，相比于上述的单卡多机它的优势是减少了网络IO的开销，只在本机内部多个GPU之间交换数据。而GPU之间往往有1GB+，甚至100GB的专有带宽。而且，相比与分布式训练带来的AUC损耗，它对AUC的影响应该也会更少些。但是通过2周多的调研，我们发现LightGBM社区多多卡模式的支持非常差。因此放弃了。 其他问题\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\n这样的文件中。 固定资源池资源申请 1、新增Lab时增加资源池类型选项，单选框 可选“内存型资源池” “计算型资源池” 等 非“公共资源池” 的类型。 （1）选择固定资源池时，提示剩余资源情况 注意：因为阿里云当前的一些策略，这里显示的配额数并不准确。申请时尽量不要把配额中剩余的资源都申请光，否则可能会出现资源不够的情况。而且如果多人同时申请，也可能导致后提交的人资源不够用。 当资源不够的时候会进入排队，实例会一直处于初始化状态。 解释一下这二者的区别，\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=80187151\\n\\n技术选型 根据上面的分析， 我们最终选择用多个单卡机器组建ray集群 ，实现分布式训练。因为相比于cpu机器它不仅总成本也不高，而且运行速度更快，因此性价比最好。具体做法是，首先构建我们自己的GPU资源池，（ 目前是10台ecs.gn7i-c32g1.8xlarge，单月成本7.02w ）当用户提交任务时，我们会优先选择私有资源池的资源，如果私有资源池没有资源了，我们才选择公有资源池的CPU机器。之所以公有资源池不选择GPU机器，是因为GPU库存经常紧张，很容易导致任务提交失败。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\n且持续30分钟 自动停止 （3）新增Lab\\xa0:点击新增，录入 名称、描述、\\xa0instancetype、 i mage\\xa0url。名称需校验唯一性。都为必填项 (api.creat_lab） 注意资源池，一般可以选公共资源池，或者非公共资源池（比如内存型资源池）。公共资源池是阿里云的后付费机器，初始化时间长（5-10min）。 非公共资源池则是我们预付费购买的资源池，初始化时间短，但是资源紧张，先到先得 （4）容器化：点击容器化按钮，弹窗展示“设置模板参数” （api.create_template) 模板名称、模板描述、配置录入 脚本path、参数配置。模板名称、模板描述脚本path为必填项\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=80187151\\n\\n(api.stop_container_job) 运行日志会保存在和脚本相同的目录下。比如脚本传入的为\\xa0/data/automl/tongxinwen/hello.ipynb，那么日志会输出到 /data/automl/tongxinwen/hell o_out_20240119_140136 .ipynb 这样的文件中。 固定资源池资源申请 1、新增Lab时增加资源池类型选项，单选框 可选“内存型资源池” “计算型资源池” 等 非“公共资源池” 的类型。 （1）选择固定资源池时，提示剩余资源情况\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=80187151\\n\\n一次性训练维度为17k，样本量80w的数据 。模型迭代2w次后收敛，大概消耗3个小时左右（花费 ￥58）。 如果使用阿里云的PAI平台，目前我们最大可以申请到3T内存的机器（公有资源池），暂时应付10w+的feature应该是可以的。 （注：我们也可以申请私有资源池，这样内存的申请应该可以做到更大。阿里可以把多台机器连起来，但是用起来就像单机，待确认） 使用介绍 脚本位置： 源代码： https://gitlab.yangqianguan.com/risk/automl_images/-/tree/master/dsw-py38 所有脚本都会通过docker打包到镜像中，镜像部署到AI\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=80898239\\n\\n（2）选择弹性资源池时，配置与现状相同 注意，使用固定资源池时，镜像会优先选择在本地缓存的版本。所以第一次拉取镜像的时间会比较长，后面就快很多了。 同样的道理，如果升级了镜像内容但是没有升级镜像版本，有可能会拉取到旧的镜像。因此建议在每次升级镜像时都要升级一下版本。 3、编辑页 初始化状态不支持编辑，运行中、停止状态可以编辑 编辑弹窗同样增加资源池类型选择 资源池类型不可修改，但相关资源配置可以修改，修改后该实例重新进入初始化状态 4、容器：启动容器时同样增加“弹性资源池”“固定资源池”选项 权限申请 在权限申请页面，\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=80187151，问题是：公共资源池和私有资源池的区别\\nAI: 根据context文本，我认为答案是：'},\n",
       " {'role': 'assistant',\n",
       "  'content': '公共资源池和私有资源池的主要区别在于资源获取方式和计费方式。\\n\\n公共资源池是阿里云的后付费资源池，用户可以根据需求申请使用，按照实际使用的小时数进行计费，不用则不计费。创建和启动实例的过程相对较慢。\\n\\n私有资源池则是用户预付费购买的资源池，创建和启动实例的速度较快。用户在使用时需要先付费购买资源，然后才能使用。相比于公共资源池，私有资源池的资源更加稳定，不会因资源紧张而导致任务提交失败。但是，如果多人同时使用私有资源池，可能会导致资源不足，需要排队等待或者进入初始化状态。因此，在选择资源池时需要根据实际需求进行选择。以上信息来源于context文本中的描述。'},\n",
       " {'role': 'user',\n",
       "  'content': 'Human: 你是一个解答用户问题的assistant，可以根据从语料库中召回的context文本回答问题。请尽量保证回答的内容都可以在context中找到根据，并务必保留 source 后面的url链接。以下是context文本：LightGBM分布式训练（CPU、GPU） 基于Ray框架 原理 Ray是一款非常通用的机器学习框架，同时支持LightGBM，XGboost，pytorch等多种模型框架的cpu，gpu及分布式训练。 它的LightGBM分布式训练方法本身也是利用了LightGBM自身的分布式训练功能，Ray只是为其提供了一套多机的分布式计算框架。关于LightGBM的分布式训练原理可以参考其官方文档： https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-distributed-learning\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155\\n\\n背景 为了加速大型数据集的LightGBM打分，我们自研了一套分布式LightGBM打分模版。它的原理非常简单，就是通过读取多个数据分片文件，然后把各个分片平均分配到多个pod，最后在每个pod上启动打分程序。所以，要想使用该模版， 需要具备2个前提条件 ，1是数据集必须是分片好的，而且分片个数等于并行pod数的整数倍，2是必须是parquet格式。为了方便大家使用，我们在automl_yqg这个包中，提供了2个函数进行分片，详见 下文 。 另外，分布式输出的打分文件是多个csv文件，可以借助automl_yqg.io中的read_dataframe这个函数进行读取。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=105312091\\n\\n另外，在master节点，我们会通过ray client提交训练任务到这个集群上。 LightGBM分布式训练 则是Ray调用LightGBM官方提供的训练API进行机器组网，并开启分布式训练。 其实如果没有ray，只要我们有几台可以网络互通的机器，也是可以做类似的事情的。 但是有了Ray，它就会帮我们进行资源管理，信号同步，日志汇总，监控采集，失败重启等各种事情。帮我们省去了很多麻烦。 如果使用dask，那结构是非常类似的。 性能测试(ray) 计算性能 下面主要分析一下分布式训练的性能提升：\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\nhttps://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-distributed-learning 目前AiLab单节点最大可以分配的cpu数是200，内存是3TB，越来越成为我们的瓶颈。而且对于LightGBM来说，当它在初始化训练时，加载数据这个环节是单线程的，非常耗时也浪费了宝贵的成本。 分布式训练时，平台会自动把训练集和测试集的数据平均分配到各个ray的worker上，大大减少了load\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155\\n\\nRay集群就是构建在DLC临时搭建的小集群上 的。我们可以根据环境变量获知哪个pod是master，哪个是worker，从而启动不同的脚本和执行不同操作。 比如在master节点我们的启动命令是 ray start --head --port\\xa0$RAY_PORT worker节点则是： ray start --address $MASTER_ADDR:$RAY_PORT 另外，在master节点，我们会通过ray client提交训练任务到这个集群上。 LightGBM分布式训练 则是Ray调用LightGBM官方提供的训练API进行机器组网，并开启分布式训练。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\n即使分布式能发挥一定提速效果，但训练轮数增加则总时长就差不多了。收益就很小了。 Ray 官网： https://www.ray.io/ 主要目的是构建一个通用的分布式的模型训练、推理、调参框架。支持数据处理，但是这些数据处理主要是服务于模型训练相关，不太适合进行通用的大数据处理任务。 而且，它在数据处理，模型训练、调参等很多方面进行了重新设计，有一定的学习成本。 它也支持分布式LightGBM训练，并参考dask的源代码，在它基础上支持了early_stop。 所以，后面我们就采用了Ray作为分布式训练框架。 系统结构 我们的系统主要分如下几个部分（以Ray为例）: Automl\\xa0→ Pai\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\n。而且GPU利用率提高了一倍。所以建议设置为16 不过，在分布式模式下，AUC不可避免的出现了下降。因为在GPU和分布式训练时，我们必须采用不同的训练算法。比如 tree_learner 为 voting ，单机tree_learner则是serial。voting相当于一个近似算法，分布式训练时每个机器上都会在自己本地数据集上进行训练，得到一棵树。然后LightGBM通过某种投票算法选出最优的一棵树，更新到最终的模型里，所以这种方法肯定有精度损失。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\n日志在/data/automl/<email-name>/<job_name>_logdir/<version>.log 基于dask框架(deprecated) 原理 LightGBM官方支持的分布式计算是基于Dask平台的，我们是借助阿里云Pai平台的DLC架构，动态的为每个job建立一个小的Dask集群，来进行分布式训练的。 Dask集群创立完成后，worker之间通过socket连接交换节点信息。而LightGBM则是在此基础上，又创立了自己的tcp网络，进行各个节点训练信息的同步。具体见LightGBM官方文档：\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155，问题是：如何进行分布式LightGBM模型训练\\nAI: 根据context文本，我认为答案是：'},\n",
       " {'role': 'assistant',\n",
       "  'content': '分布式LightGBM模型训练可以通过以下步骤进行：\\n\\n1. 数据分片：将数据集分成多个分片文件，每个分片文件包含数据的一部分。分片的数量需要是并行处理节点的整数倍。\\n2. 分布式计算框架的搭建：利用Ray等分布式计算框架，构建多机的分布式计算环境。\\n3. 提交训练任务：通过Ray框架提交LightGBM的训练任务到集群上，开启分布式训练。\\n4. 数据加载和模型训练：平台会自动将训练集和测试集的数据平均分配到各个worker节点上，然后利用LightGBM官方提供的训练API进行模型训练。在分布式训练过程中，每个机器都会在自己本地数据集上进行训练，并通过某种投票算法选出最优的模型更新到最终的模型里。\\n\\n请注意，以上步骤需要根据具体情况进行调整和优化，以确保分布式训练的效果和效率。另外，分布式训练可能会带来一些精度损失，需要根据实际需求进行权衡和选择。如有更多疑问或需求，建议参考LightGBM官方文档或咨询相关领域的专家。'},\n",
       " {'role': 'user',\n",
       "  'content': 'Human: 你是一个解答用户问题的assistant，可以根据从语料库中召回的context文本回答问题。请尽量保证回答的内容都可以在context中找到根据，并务必保留 source 后面的url链接。以下是context文本：增加一个开关结合待替换模型id列表配置和服务名称,在构建时如果符合条件,则将去查当前模型复制出来的模型id,使用复制的模型id查询特征依赖进行构建 具体判断如下 if （model-mirror && 通过model_id查询到它复制了一个新modelInfo) || (useNewFeatureListSwitch &&\\xa0toReplaceModelList.contains(modelId) )： // 查询复制的新model info，并用它构建元数据 else: // 跟原来一样 if 条件的第一部分是在mirror环境中生效，第二部分在prod环境生效。 配置 样例 说明\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=84512081\\n\\n背景 为了加速大型数据集的LightGBM打分，我们自研了一套分布式LightGBM打分模版。它的原理非常简单，就是通过读取多个数据分片文件，然后把各个分片平均分配到多个pod，最后在每个pod上启动打分程序。所以，要想使用该模版， 需要具备2个前提条件 ，1是数据集必须是分片好的，而且分片个数等于并行pod数的整数倍，2是必须是parquet格式。为了方便大家使用，我们在automl_yqg这个包中，提供了2个函数进行分片，详见 下文 。 另外，分布式输出的打分文件是多个csv文件，可以借助automl_yqg.io中的read_dataframe这个函数进行读取。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=105312091\\n\\n特殊人群资源限额，用户id维度 appliedQuotaThreshold json {\"dswQuota\":{\"cpu\":500,\"memory\":1024,\"gpu\":500},\"dlcQuota\":{\"cpu\":500,\"memory\":1024,\"gpu\":500}} 申请资源阈值，超过一定数字后需要+2 上级审批 oa.resourceFlowCode string MPQ 额外资源申请flowCode shutdownResourceJobInterval Long 240 资源不足时强制关机定时任务执行间隔，单位： min\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=103783146\\n\\n2、文件填写要求： （1）上传txt文件，一行一个特征，以\\\\t分隔。无需分箱分箱规则不填写 （2）文件从第二行开始读取 （3）示例：索引 特征名称 特征场景 默认值 是否有flag特征 分箱规则 示例：0 nifa_sharedqueryfeatureloanamount C NaN TRUE [{\"bin\":\"(0,1]\",\"score\":0.1},{\"bin\":\"(1 2]\",\"score\"0.2},{\"bin\":\"(2 3]\",\"score\"0.3}] CV模型组部署 只描述与单模型操作差异点 1、基本信息 （1）英文名称、中文名称、描述、场景、框架、类型、空值特征分数与上述单模型一致\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=85647676\\n\\n分布式训练时，平台会自动把训练集和测试集的数据平均分配到各个ray的worker上，大大减少了load data带来的开销，同时减少了单节点的内存压力，这样总内存的使用量不变，但是我们可以使用多个节点增加cpu数量，提高训练效率。比如我们可以申请10台32核，64GB的高性能机器，实现320核+640GB的训练效果。这个是单机提供不了的。 如何提交任务 前提条件 提交任务之前，我们需要具备2个条件 准备数据集。数据集的格式有要求， 必须有label列。 必须有train、valid 2个数据集。且只能有这2个 必须放到同一个父目录下。 目前我们选择的机型有限，\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155\\n\\n2、资源申请时的限制，通过角色的区分限制单次使用资源的最大值 最大资源申请数量的限制只能在一定程度上起到限制申请远大于实际使用资源数量的情况，无法降低用户在时间段内使用的总资源数量。所以现在采用个人配额管理方案来管理用户在一周内可使用的资源量。根据过往用户对CPU、内存、GPU资源使用的情况，设置一定的阈值。每个用户在周一进行可支配资源的重置，之后实时计算用户剩余可支配资源，剩余可支配资源不足时可进行额外资源申请，由直属上级审批，审批通过后方可继续使用。如资源不足后4H内未进行额外资源申请，则强行释放其下所有的资源。 基本流程 true 流程图 false auto top true 429\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=108535118\\n\\n准备数据集。数据集的格式有要求， 必须有label列。 必须有train、valid 2个数据集。且只能有这2个 必须放到同一个父目录下。 目前我们选择的机型有限， parquet格式的训练样本最好不要超过60G ，否则可能出现内存被打爆无法训练的情况。 train数据必须分片（如果使用AiLab的api提交，可以自动帮用户分片）。最终的目录形式如下：即一个repartition的目录，这是训练集，一个valid.parquet文件，这是验证集。用来辅助early stop 准备训练参数。训练参数是通过一个yaml文件定义的。示例如下： yml 注意，\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155\\n\\nAPI和SDK集成，从而使服务在创建后，可以自动注册到模型数据库中，这样决策引擎可以直接使用，不需要算法同学操作，真正实现一键式部署。 与之配套的其他功能还有镜像管理（ImageManage)，配额管理（QutoaManage），还有监控等。这些可以由我们手动先管理起来，不需要算法同学关心。 CreateService的工作流程 可以看到，CreateService主要包括5步: Validation: 包括基本信息的验证，比如service name是否符合规范，是否填写了author name，thread num是否小于cpu核数等 create service：调用PAI\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=74290682，问题是：.. 需要具备哪些前提条件\\nAI: 根据context文本，我认为答案是：'},\n",
       " {'role': 'assistant',\n",
       "  'content': '在进行分布式训练时，需要具备以下前提条件：\\n\\n1. 数据集必须是分片好的，且分片个数需要是并行处理节点的整数倍。\\n2. 数据集必须是parquet格式。\\n3. 训练集和测试集的数据需要自动分配到各个处理节点上，以减少load data带来的开销和单节点的内存压力。\\n4. 需要准备训练参数，这些参数通过yaml文件定义。\\n5. 在使用某些服务或功能时，可能需要用户的label列、train和valid数据集，且这些数据集必须放在同一个父目录下。\\n6. 对于使用AiLab的api提交的情况，训练样本的parquet格式最好不要超过60G，否则可能会出现内存不足无法训练的情况。\\n\\n以上是根据context文本提供的信息整理的需要具备的前提条件，如果还有其他细节要求或特定环境的要求，请进一步确认。'},\n",
       " {'role': 'user',\n",
       "  'content': 'Human: 你是一个解答用户问题的assistant，可以根据从语料库中召回的context文本回答问题。请尽量保证回答的内容都可以在context中找到根据，并务必保留 source 后面的url链接。以下是context文本：当用户想运行这些模版的时候，则需要指定具体参数值，对于可选参数，此时就使用默认值。对于必选参数，如果不填，则会报错。 在执行容器，框架会把参数注入到命令行中： 比如`-p learning_rate 0.1 -p data_path xxxxx` 所以，用户需要保证他的脚本可以接受这样的参数。 如果脚本是ipynb，系统会使用papermill执行，此时可以参考其官网教程，把 某些包含参数的cell打上tag为parameters即可 ，不再需要额外设计参数解析的逻辑， 具体参考：\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=76361810\\n\\nLR模型进行label预估， 然后选出feature importance排在前面的 top3000 维 feature。 （如果选feature importance 大于0的feature，那么2种方式会选出7000+feature，在3T内存的机器上都跑不通。所以暂时选3000维） 3 模型重训和HPO自动化超参数调优 抽取完有用的feature之后，再对所有样本进行重训。重训的时候采取了2种方式，一是采用写死的参数重训，一种是采用HPO自动帮我们搜索最优参数。 写死的模型训练参数如下： yml HPO自动寻参的参数如下： yml 实验结果 写死参数的模型还没有跑出来\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=85651968\\n\\n（4）容器化：点击容器化按钮，弹窗展示“设置模板参数” （api.create_template) 模板名称、模板描述、配置录入 脚本path、参数配置。模板名称、模板描述脚本path为必填项 。 注意脚本路径应该是以/home/automl，/data/public_data或者/data/automl/开头的路径，否则容器会找不到脚本，导致任务失败 。 同时，系统目前支持.ipynb, .py和.sh 3种格式的脚本作为模版。 在执行容器，平台会把参数注入到命令行中： 比如`-p learning_rate 0.1 -p data_path xxxxx`\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=80187151\\n\\n通过目前的HPO探索结果（150轮trials）显示，在分布式模式下，我们可以适当增加max_depth、num_leaves等参数，减少reg_alpha，reg_lambda。不过需要注意，增加max_depth等参数后会明显增加训练时长。 这些参数都和过拟合有些关系。通常如果我们想减少过拟合，会采用减少max_depth，增加reg_lambda等手段，而现在我们则需要适当反向调节。所以，我们可以推测，分布式训练其实已经起到了一定程度上抑制过拟合的效果，此时就可以把相关参数反向调节一下，应该可以达到和原来单机训练类似的AUC。 技术选型 根据上面的分析，\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\n400000GiB*小时 （后续可根据资源使用率和使用量调整） 注 ailab和容器分开统计计算配额，所以此处ailab和容器额分别为CPU 15000核*小时 、内存 400000GiB*小时每周 对于数字的单位，比如cpu的默认值15000核*小时，如果用户申请了50核的机器，那么按照现有配额，他可以持续使用该机器300H，100核的机器则可以使用150H。内存、GPU类似。 2. 如果配额用完了或剩余配额小于本次申请资源数，新建AILab和启动容器模板提交时提示“本周资源配额已不足，请申请资源后再使用”。 3. 额外资源申请 （1）进入AI资源管理-权限申请菜单\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=109348485\\n\\nparam中的yaml文件。这个yaml文件定义了需要探索的参数列表。具体参数含义见下面 hpo yaml配置 一个具体的例子如下：关键参数的含义已经在注释中写明。 yml target py 该文件定义了我们需要优化的目标，它必须包含一个名字为target的函数，如下 它有几个注意点： 第一个参数必须是名为id的参数 必须返回一个metric，这个metric会用来对比不同trial的效果，Optuna会分析当前所有的trial结果，并且通过内置的 TPE算法 ，选择下一个它认为可能较好的参数集进行新的尝试 参数名字必须和上面hpo\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=85650497\\n\\nhttps://www.tapd.cn/49918899/prong/stories/view/1149918899001071682 关于超参数调优， 举个例子，目前首贷同学面临的一个常见的并行计算任务是，给定一个数据集，他们希望选出一个时间段，在该时间段内模型表现最好，然后使用这段数据集重训模型。这个本质上是一个超参数调优的工作。很多超参数都可以通过并行计算加速，并利用成熟的Search算法（比如贝叶斯优化，GridSearch等）选择最优值。这比手动选择，或者直接使用某些经验值更加合理且高效。所以我们希望提供一个通用的超参数调优框架，方便算法同学在各个场景使用。\\nsource: https://wiki.fintopia.tech/display/riskDev/2024H1\\n\\nhttps://www.tapd.cn/49918899/prong/stories/view/1149918899001071682 关于超参数调优， 举个例子，目前首贷同学面临的一个常见的并行计算任务是，给定一个数据集，他们希望选出一个时间段，在该时间段内模型表现最好，然后使用这段数据集重训模型。这个本质上是一个超参数调优的工作。很多超参数都可以通过并行计算加速，并利用成熟的Search算法（比如贝叶斯优化，GridSearch等）选择最优值。这比手动选择，或者直接使用某些经验值更加合理且高效。所以我们希望提供一个通用的超参数调优框架，方便算法同学在各个场景使用。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=78272796，问题是：.. 参数建议如何设置\\nAI: 根据context文本，我认为答案是：'},\n",
       " {'role': 'assistant',\n",
       "  'content': '参数建议的设置需要根据具体情况而定，对于可选参数，可以使用默认值；对于必选参数，则需要按照要求填写具体的参数值。\\n\\n在执行容器时，框架会把参数注入到命令行中，用户需要保证脚本可以接受这样的参数。对于参数的具体设置，可以参考相关官方文档或教程，如Papermill的官网教程。此外，根据实验结果和模型表现，可以对参数进行调整和优化，例如通过HPO自动寻参的方式，自动搜索最优参数。\\n\\n对于超参数调优，可以使用成熟的Search算法（如贝叶斯优化、GridSearch等）选择最优值。在设置参数时，需要根据具体的模型和任务来确定哪些参数需要调整，以及调整的范围和步长等。\\n\\n以上信息仅供参考，具体的参数设置建议还需要根据实际需求和实践经验进行调整。'}]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING][2024-10-15 23:57:49.892] base.py:916 [t:8703163072]: This key `disable_search` does not seem to be a parameter that the model `ERNIE-Speed-128K` will accept\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ailab实例一直黑屏大部分情况下，用户打开AILab应该可以直接进入Notebook页面。不过偶尔会卡在上述页面，此时点击一下右上角的关闭按钮即可。有的时候是因为登录过期了。这个字显示的非常不明显。需要自己看看。'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"ailab实例一直黑屏\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['如何进行分布式LightGBM模型训练',\n",
       " '进行分布式LightGBM模型训练，可以基于Ray框架来实现。Ray是一款支持LightGBM等多种模型框架的分布式训练机器学习框架。它利用LightGBM自身的分布式训练功能，为LightGBM提供了一套多机的分布式计算框架。在分布式训练时，Ray会调用LightGBM官方提供的训练API进行机器组网，并开启分布式训练。此外，为了提高GPU利用率和训练效率，可以在分布式模式下采用特定的训练算法，如voting。但需要注意，分布式训练可能会导致一定的精度损失。更多细节和原理可以参考LightGBM的官方文档：https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-distributed-learning 。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155、https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570',\n",
       " '需要具备哪些前提条件',\n",
       " '为了使用自研的分布式LightGBM打分模版，需要具备以下两个前提条件：1是数据集必须是分片好的，而且分片个数等于并行pod数的整数倍；2是数据集必须是parquet格式。详情请参考：https://wiki.fintopia.tech/pages/viewpage.action?pageId=105312091',\n",
       " '参数建议如何设置',\n",
       " '参数设置需要根据具体的模型训练任务和数据集特性来决定。在LightGBM的分布式训练中，可以考虑调整诸如\"bagging_freq\", \"max_bin\", \"min_data_in_bin\", \"min_data_in_leaf\", \"min_sum_hessian_in_leaf\", \"min_child_weight\", \"boosting_type\", \"objective\", \"metric\"等参数。具体的参数值需要通过实验和调优来确定，可以使用Optuna等调参工具来帮助选择较优的参数组合。并没有固定的参数设置建议，因为每个数据集和任务都是独特的。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=85651968、https://wiki.fintopia.tech/pages/viewpage.action?pageId=8565049']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_ans = [c['content'] for c in llm.conversation_history if c['role'] == 'assistant']\n",
    "user_quest = llm.user_question_history\n",
    "qa_pairs = []\n",
    "for q, a in zip(user_quest, ai_ans):\n",
    "    qa_pairs += [q, a]\n",
    "qa_pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-wechat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
