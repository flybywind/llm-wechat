{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "from secret import AK_SK\n",
    "keys = AK_SK('../qianfan.keys')\n",
    "os.environ[\"QIANFAN_ACCESS_KEY\"] = keys.get_ak()\n",
    "os.environ[\"QIANFAN_SECRET_KEY\"] = keys.get_sk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING][2024-10-15 21:44:02.194] redis_rate_limiter.py:21 [t:8703163072]: no redis installed, RedisRateLimiter unavailable\n"
     ]
    }
   ],
   "source": [
    "from myqianfan import QianfanLLM, QianfanEmbedding\n",
    "from myqianfan import model_spec\n",
    "\n",
    "llm = QianfanLLM(model_spec=model_spec.ERNIE4_Turbo8K, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlassian import Confluence\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers.transform import BaseTransformOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ConfluenceLoader\n",
    "from langchain.schema import BaseOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\"qianfan\", embedding_function=QianfanEmbedding(), persist_directory=\"vectorstore/fintopia.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一次初始化时运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding documents 0 to 16\n",
      "Adding documents 16 to 32\n",
      "Adding documents 32 to 48\n",
      "Adding documents 48 to 64\n",
      "Adding documents 64 to 80\n",
      "Adding documents 80 to 96\n",
      "Adding documents 96 to 112\n",
      "Adding documents 112 to 128\n",
      "Adding documents 128 to 144\n",
      "Adding documents 144 to 160\n",
      "Adding documents 160 to 176\n",
      "Adding documents 176 to 192\n",
      "Adding documents 192 to 208\n",
      "Adding documents 208 to 224\n",
      "Adding documents 224 to 240\n",
      "Adding documents 240 to 256\n",
      "Adding documents 256 to 272\n",
      "Adding documents 272 to 288\n",
      "Adding documents 288 to 304\n",
      "Adding documents 304 to 320\n",
      "Adding documents 320 to 336\n",
      "Adding documents 336 to 352\n",
      "Adding documents 352 to 368\n",
      "Adding documents 368 to 384\n",
      "Adding documents 384 to 400\n",
      "Adding documents 400 to 416\n",
      "Adding documents 416 to 432\n",
      "Adding documents 432 to 448\n",
      "Adding documents 448 to 464\n",
      "Adding documents 464 to 480\n",
      "Adding documents 480 to 496\n",
      "Adding documents 496 to 512\n"
     ]
    }
   ],
   "source": [
    "def get_children_pages_recursively(client, page_id: str):\n",
    "    child_pages = client.get_page_child_by_type(page_id)\n",
    "    for page in child_pages:\n",
    "        yield page\n",
    "        if \"id\" in page:\n",
    "            yield from get_children_pages_recursively(client, page[\"id\"])\n",
    "\n",
    "from getpass import getpass\n",
    "wiki_psw = getpass(\"wiki:\")\n",
    "# get all pages under some page\n",
    "confluence = Confluence(\n",
    "    url='https://wiki.fintopia.tech/',\n",
    "    username='tongxinwen@fintopia.tech',\n",
    "    password=wiki_psw)\n",
    "child_pages = [p for p in get_children_pages_recursively(confluence, \"74290517\")]\n",
    "\n",
    "loader = ConfluenceLoader(\n",
    "    url=\"https://wiki.fintopia.tech/\",\n",
    "    page_ids=[p[\"id\"] for p in child_pages],\n",
    "    username=\"tongxinwen@fintopia.tech\",\n",
    "    api_key=wiki_psw,\n",
    "    limit=10,\n",
    ")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(documents)\n",
    "\n",
    "batch_size = 16\n",
    "doc_ids = []\n",
    "for i in range(0, len(splits), batch_size):\n",
    "    print(f\"Adding documents {i} to {i+batch_size}\")\n",
    "    texts = [doc.page_content for doc in splits[i:i+batch_size]]\n",
    "    metadatas = [doc.metadata for doc in splits[i:i+batch_size]]\n",
    "    doc_ids += vectorstore.add_texts(texts, metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': '95160416', 'source': 'https://wiki.fintopia.tech/display/riskDev/2024H2', 'title': '2024H2', 'when': '2024-07-09T17:39:33.000+08:00'}, page_content='200ms KR3 模型推理框架国内外统一 【P2】 KR4 模型cache表优化，提高插入和查询效率，同时降低存储压力【P2】 建立HBase + Hive + PolarDB 3位一体的多级缓存 查询方面通过PolarDB + HBase提供高效查询，存储方面通过Hive提供长期存储功能，方面回溯各种问题 建立各级存储模块的过期策略，在PolarDB中存储原来的30%的数据，HBase存储未过期model的所有记录，hive存储最近3年的所有记录 O5 海外平台搭建 【P0】 以菲律宾、印尼为主 目标，训练全部迁移到ailab，推理迁移到新框架。'),\n",
       " Document(metadata={'id': '95160416', 'source': 'https://wiki.fintopia.tech/display/riskDev/2024H2', 'title': '2024H2', 'when': '2024-07-09T17:39:33.000+08:00'}, page_content='200ms KR3 模型推理框架国内外统一 【P2】 KR4 模型cache表优化，提高插入和查询效率，同时降低存储压力【P2】 建立HBase + Hive + PolarDB 3位一体的多级缓存 查询方面通过PolarDB + HBase提供高效查询，存储方面通过Hive提供长期存储功能，方面回溯各种问题 建立各级存储模块的过期策略，在PolarDB中存储原来的30%的数据，HBase存储未过期model的所有记录，hive存储最近3年的所有记录 O5 海外平台搭建 【P0】 以菲律宾、印尼为主 目标，训练全部迁移到ailab，推理迁移到新框架。'),\n",
       " Document(metadata={'id': '91856753', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=91856753', 'title': '7. 在AiLab中连接hive', 'when': '2024-09-03T14:48:03.000+08:00'}, page_content='因为权限管理和LDAP一样，所以用户如果想要申请某个表的权限，需要去数仓申请 https://dw-lighthouse.fintopia.tech/tools/hive-permission 另外，如果不想把自己的密码保存在ipynb中，可以使用getpass函数，如下，这样就保存在passwd这个变量里了 ！！注意 ！！ hive.read_sql 这句话会把整个表下载到本地。强烈建议先在SQL中对数据进行过滤，采样或者group。 比如如果想知道某个表的行数，我们应该这样做： py 而不是这样：否则很可能把AiLab打挂 py 向hive中导入表 py \",'),\n",
       " Document(metadata={'id': '91856753', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=91856753', 'title': '7. 在AiLab中连接hive', 'when': '2024-09-03T14:48:03.000+08:00'}, page_content='因为权限管理和LDAP一样，所以用户如果想要申请某个表的权限，需要去数仓申请 https://dw-lighthouse.fintopia.tech/tools/hive-permission 另外，如果不想把自己的密码保存在ipynb中，可以使用getpass函数，如下，这样就保存在passwd这个变量里了 ！！注意 ！！ hive.read_sql 这句话会把整个表下载到本地。强烈建议先在SQL中对数据进行过滤，采样或者group。 比如如果想知道某个表的行数，我们应该这样做： py 而不是这样：否则很可能把AiLab打挂 py 向hive中导入表 py \",')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorstore.search(\"数据库\", search_type=\"similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Rag chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"human\", \"你是一个解答用户问题的assistant，可以根据从语料库中召回的context文本回答问题。请尽量保证回答的内容都可以在context中找到根据，并务必保留 source 后面的url链接。\\n以下是context文本：{context}，\\n问题是：{question}\"),\n",
    "            (\"assistant\", \"根据context文本，我认为答案是：\"),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': '91856753', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=91856753', 'title': '7. 在AiLab中连接hive', 'when': '2024-09-03T14:48:03.000+08:00'}, page_content='因为权限管理和LDAP一样，所以用户如果想要申请某个表的权限，需要去数仓申请 https://dw-lighthouse.fintopia.tech/tools/hive-permission 另外，如果不想把自己的密码保存在ipynb中，可以使用getpass函数，如下，这样就保存在passwd这个变量里了 ！！注意 ！！ hive.read_sql 这句话会把整个表下载到本地。强烈建议先在SQL中对数据进行过滤，采样或者group。 比如如果想知道某个表的行数，我们应该这样做： py 而不是这样：否则很可能把AiLab打挂 py 向hive中导入表 py \",'),\n",
       " Document(metadata={'id': '97182399', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=97182399', 'title': '模型迁移ModelServe', 'when': '2024-09-09T19:29:40.000+08:00'}, page_content='优缺点:直接读库开发简单,但是需要频繁扫model_result大表(百亿级),可能会有性能问题. 方案一:直接通过hivesql,通过hive直接对比增量数据 优缺点:通过hive的资源去做对比避免影响线上mysql性能,但是需要注意大批量的对比sql可能会超时. 5.一键切换模型服务到新 通过mirror服务核对后,确认模型迁移后无diff,需要提供一键转换功能,将模型的部署方式转换为ModelServe部署,主要更新MODEL_DATA中的deployType为3.然后清楚掉模型的缓存,让新的预测重新从数据库加载进行'),\n",
       " Document(metadata={'id': '97182399', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=97182399', 'title': '模型迁移ModelServe', 'when': '2024-09-09T19:29:40.000+08:00'}, page_content='模型多框架同时注册并开启陪跑后,model_result_mirror表会存模型转写新框架的预测结果,通过和model_result表线上结果做核对,来确认转写的模型是否可迁移. 新建job,定时核对mirror预测结果和线上结果diff 方案一:增量扫描model_result_mirror表数据,然后根据结果去查询model_result表直接进行核对(注意分批查询,使用trace和model的联合索引) 优缺点:直接读库开发简单,但是需要频繁扫model_result大表(百亿级),可能会有性能问题. 方案一:直接通过hivesql,通过hive直接对比增量数据'),\n",
       " Document(metadata={'id': '99949500', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=99949500', 'title': '2024-07. DAG流水线支持', 'when': '2024-08-09T13:58:10.000+08:00'}, page_content='}]]> mysql 表定义 sql 关键函数介绍 pipeline相关 创建pipeline PipelineDataVO createNew (PipelineDataVO pipelineDataVO) 用于第一次创建一个新流图。会在pipeline表中创建一个空图，并返回其pipeline id。 编辑pipeline boolean beforeEdit (long pplId， String uerId)\\xa0// 返回是否在草图表中已经有了 PipelineDataVO startEdit (long pplId, String userId, boolean'),\n",
       " Document(metadata={'id': '91856753', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=91856753', 'title': '7. 在AiLab中连接hive', 'when': '2024-09-03T14:48:03.000+08:00'}, page_content='执行SQL 目前我们已经把AILab的网络和hive的主机打通，用户可以直接在AiLab中访问hive，并转化成pandas的DataFrame进行使用。 具体使用方法如下： 使用dsw-lgb-xgb-py38:v1.12及以上版本的镜像 创建ailab。具体见 打开jupyter，demo如下 py 运行结果如下： 因为权限管理和LDAP一样，所以用户如果想要申请某个表的权限，需要去数仓申请 https://dw-lighthouse.fintopia.tech/tools/hive-permission'),\n",
       " Document(metadata={'id': '78279979', 'source': 'https://wiki.fintopia.tech/pages/viewpage.action?pageId=78279979', 'title': '数据探查任务集成到etl pipeline', 'when': '2024-07-04T14:48:07.000+08:00'}, page_content='RUNNING\\xa0→ COMPLETED\\xa0→ HIVE_FINISHED → DW_PUSHING 3 方案提议 3.1 数据探查服务 从第二部分的描述可知，最终etl job都是通过automl scheduler的一个离线job进行状态检查和流转的。这一步逻辑是统一的。所以可以把我们的数据探查任务分别加入HiveQueryExportFeatureStatusJob和FeatureExportTaskScanJob这2个job，当任务状态变成HIVE_FINISHED后，调用数仓的一个接口，比如我们可以创建一个单独的接口')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.search(\"数据库\", search_type=\"similarity\", k=6)\n",
    "# vectorstore.search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import Field\n",
    "\n",
    "class VectorStoreRetrieverMoreHistory(BaseRetriever):\n",
    "    vs: VectorStore\n",
    "    llm: LLM\n",
    "    topk: int = Field(gt=0, default=5, description=\"Number of documents to retrieve\")\n",
    "\n",
    "    def _get_relevant_documents(\n",
    "        self, chat_str: str, *, run_manager):\n",
    "        ai_ans = [c['content'] for c in self.llm.conversation_history if c['role'] == 'assistant']\n",
    "        user_quest = self.llm.user_question_history\n",
    "        qa_pairs = []\n",
    "        for q, a in zip(user_quest, ai_ans):\n",
    "            qa_pairs += [q, a]\n",
    "            \n",
    "        if chat_str.startswith(\".. \"):\n",
    "        # append the last 2 conversation \n",
    "            query = \"\\n\".join(qa_pairs[-2:] + [chat_str[3:]])\n",
    "            self.llm.add_user_question(chat_str[3:])\n",
    "        elif chat_str.startswith(\".... \"):\n",
    "            # append all conversation history\n",
    "            query = \"\\n\".join(qa_pairs + [chat_str[5:]])\n",
    "            self.llm.add_user_question(chat_str[5:])\n",
    "        else:\n",
    "            self.llm.clear_history()\n",
    "            query = chat_str\n",
    "            self.llm.add_user_question(query)\n",
    "        return self.vs.similarity_search(query, k=self.topk)\n",
    "\n",
    "retriever = VectorStoreRetrieverMoreHistory(vs=vectorstore, llm=llm, topk=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(f\"{d.page_content}\\nsource: {d.metadata['source']}\" for d in docs) \n",
    "\n",
    "llm.clear_history()\n",
    "    \n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'公共资源池和私有资源池的区别主要在于计费方式、资源分配速度和使用优先级。\\n\\n1. 计费方式：\\n  * 公共资源池是后付费的，根据小时数计费，不用不计费。\\n  * 私有资源池是预付费的，即钱已经花出去了，不用就浪费了。source:https://wiki.fintopia.tech/pages/viewpage.action?pageId=80187151\\n2. 资源分配速度：\\n  * 公共资源池：创建和启动实例比较慢。\\n  * 私有资源池：创建和启动实例快。source:https://wiki.fintopia.tech/pages/viewpage.action?pageId=80187151\\n3. 使用优先级：\\n  * 当用户提交任务时，系统会优先选择私有资源池的资源，如果私有资源池没有资源了，才会选择公有资源池的CPU机器。source:https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"公共资源池和私有资源池的区别\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'进行分布式LightGBM模型训练，可以基于Ray框架来实现。Ray是一款支持LightGBM等多种模型框架的分布式训练机器学习框架。它利用LightGBM自身的分布式训练功能，为LightGBM提供了一套多机的分布式计算框架。在分布式训练时，Ray会调用LightGBM官方提供的训练API进行机器组网，并开启分布式训练。此外，为了提高GPU利用率和训练效率，可以在分布式模式下采用特定的训练算法，如voting。但需要注意，分布式训练可能会导致一定的精度损失。更多细节和原理可以参考LightGBM的官方文档：https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-distributed-learning 。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155、https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"如何进行分布式LightGBM模型训练\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'为了使用自研的分布式LightGBM打分模版，需要具备以下两个前提条件：1是数据集必须是分片好的，而且分片个数等于并行pod数的整数倍；2是数据集必须是parquet格式。详情请参考：https://wiki.fintopia.tech/pages/viewpage.action?pageId=105312091'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\".. 需要具备哪些前提条件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'参数设置需要根据具体的模型训练任务和数据集特性来决定。在LightGBM的分布式训练中，可以考虑调整诸如\"bagging_freq\", \"max_bin\", \"min_data_in_bin\", \"min_data_in_leaf\", \"min_sum_hessian_in_leaf\", \"min_child_weight\", \"boosting_type\", \"objective\", \"metric\"等参数。具体的参数值需要通过实验和调优来确定，可以使用Optuna等调参工具来帮助选择较优的参数组合。并没有固定的参数设置建议，因为每个数据集和任务都是独特的。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=85651968、https://wiki.fintopia.tech/pages/viewpage.action?pageId=85650497'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\".. 参数建议如何设置\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Human: 你是一个解答用户问题的assistant，可以根据从语料库中召回的context文本回答问题。请尽量保证回答的内容都可以在context中找到根据，并务必保留 source 后面的url链接。以下是context文本：LightGBM分布式训练（CPU、GPU） 基于Ray框架 原理 Ray是一款非常通用的机器学习框架，同时支持LightGBM，XGboost，pytorch等多种模型框架的cpu，gpu及分布式训练。 它的LightGBM分布式训练方法本身也是利用了LightGBM自身的分布式训练功能，Ray只是为其提供了一套多机的分布式计算框架。关于LightGBM的分布式训练原理可以参考其官方文档： https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-distributed-learning\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155\\n\\n背景 为了加速大型数据集的LightGBM打分，我们自研了一套分布式LightGBM打分模版。它的原理非常简单，就是通过读取多个数据分片文件，然后把各个分片平均分配到多个pod，最后在每个pod上启动打分程序。所以，要想使用该模版， 需要具备2个前提条件 ，1是数据集必须是分片好的，而且分片个数等于并行pod数的整数倍，2是必须是parquet格式。为了方便大家使用，我们在automl_yqg这个包中，提供了2个函数进行分片，详见 下文 。 另外，分布式输出的打分文件是多个csv文件，可以借助automl_yqg.io中的read_dataframe这个函数进行读取。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=105312091\\n\\n另外，在master节点，我们会通过ray client提交训练任务到这个集群上。 LightGBM分布式训练 则是Ray调用LightGBM官方提供的训练API进行机器组网，并开启分布式训练。 其实如果没有ray，只要我们有几台可以网络互通的机器，也是可以做类似的事情的。 但是有了Ray，它就会帮我们进行资源管理，信号同步，日志汇总，监控采集，失败重启等各种事情。帮我们省去了很多麻烦。 如果使用dask，那结构是非常类似的。 性能测试(ray) 计算性能 下面主要分析一下分布式训练的性能提升：\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\nhttps://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-distributed-learning 目前AiLab单节点最大可以分配的cpu数是200，内存是3TB，越来越成为我们的瓶颈。而且对于LightGBM来说，当它在初始化训练时，加载数据这个环节是单线程的，非常耗时也浪费了宝贵的成本。 分布式训练时，平台会自动把训练集和测试集的数据平均分配到各个ray的worker上，大大减少了load\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155\\n\\nRay集群就是构建在DLC临时搭建的小集群上 的。我们可以根据环境变量获知哪个pod是master，哪个是worker，从而启动不同的脚本和执行不同操作。 比如在master节点我们的启动命令是 ray start --head --port\\xa0$RAY_PORT worker节点则是： ray start --address $MASTER_ADDR:$RAY_PORT 另外，在master节点，我们会通过ray client提交训练任务到这个集群上。 LightGBM分布式训练 则是Ray调用LightGBM官方提供的训练API进行机器组网，并开启分布式训练。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\n即使分布式能发挥一定提速效果，但训练轮数增加则总时长就差不多了。收益就很小了。 Ray 官网： https://www.ray.io/ 主要目的是构建一个通用的分布式的模型训练、推理、调参框架。支持数据处理，但是这些数据处理主要是服务于模型训练相关，不太适合进行通用的大数据处理任务。 而且，它在数据处理，模型训练、调参等很多方面进行了重新设计，有一定的学习成本。 它也支持分布式LightGBM训练，并参考dask的源代码，在它基础上支持了early_stop。 所以，后面我们就采用了Ray作为分布式训练框架。 系统结构 我们的系统主要分如下几个部分（以Ray为例）: Automl\\xa0→ Pai\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\n。而且GPU利用率提高了一倍。所以建议设置为16 不过，在分布式模式下，AUC不可避免的出现了下降。因为在GPU和分布式训练时，我们必须采用不同的训练算法。比如 tree_learner 为 voting ，单机tree_learner则是serial。voting相当于一个近似算法，分布式训练时每个机器上都会在自己本地数据集上进行训练，得到一棵树。然后LightGBM通过某种投票算法选出最优的一棵树，更新到最终的模型里，所以这种方法肯定有精度损失。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\n日志在/data/automl/<email-name>/<job_name>_logdir/<version>.log 基于dask框架(deprecated) 原理 LightGBM官方支持的分布式计算是基于Dask平台的，我们是借助阿里云Pai平台的DLC架构，动态的为每个job建立一个小的Dask集群，来进行分布式训练的。 Dask集群创立完成后，worker之间通过socket连接交换节点信息。而LightGBM则是在此基础上，又创立了自己的tcp网络，进行各个节点训练信息的同步。具体见LightGBM官方文档：\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155，问题是：如何进行分布式LightGBM模型训练\\nAI: 根据context文本，我认为答案是：'},\n",
       " {'role': 'assistant',\n",
       "  'content': '进行分布式LightGBM模型训练，可以借助Ray框架来实现。Ray是一款支持LightGBM等多种模型框架的分布式训练的工具，它利用LightGBM自身的分布式训练功能，并提供了一套多机的分布式计算框架。在Ray集群中，可以通过Ray client提交训练任务，Ray会调用LightGBM官方提供的训练API进行机器组网，并开启分布式训练。同时，需要注意数据集的准备，必须是分片好的，且分片个数等于并行pod数的整数倍，并且是parquet格式。这样可以有效地进行分布式LightGBM模型训练。source: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570，https://wiki.fintopia.tech/pages/viewpage.action?pageId=9718615'},\n",
       " {'role': 'user',\n",
       "  'content': 'Human: 你是一个解答用户问题的assistant，可以根据从语料库中召回的context文本回答问题。请尽量保证回答的内容都可以在context中找到根据，并务必保留 source 后面的url链接。以下是context文本：LightGBM分布式训练（CPU、GPU） 基于Ray框架 原理 Ray是一款非常通用的机器学习框架，同时支持LightGBM，XGboost，pytorch等多种模型框架的cpu，gpu及分布式训练。 它的LightGBM分布式训练方法本身也是利用了LightGBM自身的分布式训练功能，Ray只是为其提供了一套多机的分布式计算框架。关于LightGBM的分布式训练原理可以参考其官方文档： https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-distributed-learning\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155\\n\\n另外，在master节点，我们会通过ray client提交训练任务到这个集群上。 LightGBM分布式训练 则是Ray调用LightGBM官方提供的训练API进行机器组网，并开启分布式训练。 其实如果没有ray，只要我们有几台可以网络互通的机器，也是可以做类似的事情的。 但是有了Ray，它就会帮我们进行资源管理，信号同步，日志汇总，监控采集，失败重启等各种事情。帮我们省去了很多麻烦。 如果使用dask，那结构是非常类似的。 性能测试(ray) 计算性能 下面主要分析一下分布式训练的性能提升：\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\n即使分布式能发挥一定提速效果，但训练轮数增加则总时长就差不多了。收益就很小了。 Ray 官网： https://www.ray.io/ 主要目的是构建一个通用的分布式的模型训练、推理、调参框架。支持数据处理，但是这些数据处理主要是服务于模型训练相关，不太适合进行通用的大数据处理任务。 而且，它在数据处理，模型训练、调参等很多方面进行了重新设计，有一定的学习成本。 它也支持分布式LightGBM训练，并参考dask的源代码，在它基础上支持了early_stop。 所以，后面我们就采用了Ray作为分布式训练框架。 系统结构 我们的系统主要分如下几个部分（以Ray为例）: Automl\\xa0→ Pai\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\nRay集群就是构建在DLC临时搭建的小集群上 的。我们可以根据环境变量获知哪个pod是master，哪个是worker，从而启动不同的脚本和执行不同操作。 比如在master节点我们的启动命令是 ray start --head --port\\xa0$RAY_PORT worker节点则是： ray start --address $MASTER_ADDR:$RAY_PORT 另外，在master节点，我们会通过ray client提交训练任务到这个集群上。 LightGBM分布式训练 则是Ray调用LightGBM官方提供的训练API进行机器组网，并开启分布式训练。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\n背景 为了加速大型数据集的LightGBM打分，我们自研了一套分布式LightGBM打分模版。它的原理非常简单，就是通过读取多个数据分片文件，然后把各个分片平均分配到多个pod，最后在每个pod上启动打分程序。所以，要想使用该模版， 需要具备2个前提条件 ，1是数据集必须是分片好的，而且分片个数等于并行pod数的整数倍，2是必须是parquet格式。为了方便大家使用，我们在automl_yqg这个包中，提供了2个函数进行分片，详见 下文 。 另外，分布式输出的打分文件是多个csv文件，可以借助automl_yqg.io中的read_dataframe这个函数进行读取。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=105312091\\n\\n所以，后面我们就采用了Ray作为分布式训练框架。 系统结构 我们的系统主要分如下几个部分（以Ray为例）: Automl\\xa0→ Pai DLC\\xa0→ Ray → LightGBM分布式训练 其中， Automl是前端 （包括web和sdk），方便用户提交任务和管理模版 DLC是资源编排系统 ，它借助阿里云的容器服务，可以灵活的帮我们申请多台pod，指派1个master和多个worker节点，组建网络互联环境，并搭建临时集群。 Ray集群就是构建在DLC临时搭建的小集群上 的。我们可以根据环境变量获知哪个pod是master，哪个是worker，从而启动不同的脚本和执行不同操作。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\n通过web页面提交(不推荐） 首先，我们已经创建了对应的模版 ray_lightgbm_train_v2 ，这个模版由我们平台提供维护，大家只要使用即可 上图显示了如何启动模版。 各参数的意义如下： conf\\xa0，这个参数指定了一个yaml文件，其中包含了所有训练模型需要的参数。 input_dir，训练集和验证集的父目录，必须是NAS盘绝对路径，必填 train_pat，训练集通配符，比如 train/*.parquet，train-*.parquet等，注意，这里不要把input_dir也配置进去。 valid_pat，验证集通配符，比如\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155\\n\\n分布式训练时，平台会自动把训练集和测试集的数据平均分配到各个ray的worker上，大大减少了load data带来的开销，同时减少了单节点的内存压力，这样总内存的使用量不变，但是我们可以使用多个节点增加cpu数量，提高训练效率。比如我们可以申请10台32核，64GB的高性能机器，实现320核+640GB的训练效果。这个是单机提供不了的。 如何提交任务 前提条件 提交任务之前，我们需要具备2个条件 准备数据集。数据集的格式有要求， 必须有label列。 必须有train、valid 2个数据集。且只能有这2个 必须放到同一个父目录下。 目前我们选择的机型有限，\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155，问题是：.. 需要具备哪些前提条件\\nAI: 根据context文本，我认为答案是：'},\n",
       " {'role': 'assistant',\n",
       "  'content': '进行分布式LightGBM模型训练需要具备以下前提条件：\\n1. 数据集必须是分片好的，且分片个数等于并行pod数的整数倍。\\n2. 数据集必须是parquet格式。\\n3. 数据集的格式必须有label列，且有train、valid两个数据集，且只能有这两个，必须放到同一个父目录下。source: https://wiki.fintopia.tech/pages/viewpage.action?pageId=105312091，https://wiki.fintopia.tech/pages/viewpage.action?pageId=9718615'},\n",
       " {'role': 'user',\n",
       "  'content': 'Human: 你是一个解答用户问题的assistant，可以根据从语料库中召回的context文本回答问题。请尽量保证回答的内容都可以在context中找到根据，并务必保留 source 后面的url链接。以下是context文本：背景 为了加速大型数据集的LightGBM打分，我们自研了一套分布式LightGBM打分模版。它的原理非常简单，就是通过读取多个数据分片文件，然后把各个分片平均分配到多个pod，最后在每个pod上启动打分程序。所以，要想使用该模版， 需要具备2个前提条件 ，1是数据集必须是分片好的，而且分片个数等于并行pod数的整数倍，2是必须是parquet格式。为了方便大家使用，我们在automl_yqg这个包中，提供了2个函数进行分片，详见 下文 。 另外，分布式输出的打分文件是多个csv文件，可以借助automl_yqg.io中的read_dataframe这个函数进行读取。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=105312091\\n\\n通过web页面提交(不推荐） 首先，我们已经创建了对应的模版 ray_lightgbm_train_v2 ，这个模版由我们平台提供维护，大家只要使用即可 上图显示了如何启动模版。 各参数的意义如下： conf\\xa0，这个参数指定了一个yaml文件，其中包含了所有训练模型需要的参数。 input_dir，训练集和验证集的父目录，必须是NAS盘绝对路径，必填 train_pat，训练集通配符，比如 train/*.parquet，train-*.parquet等，注意，这里不要把input_dir也配置进去。 valid_pat，验证集通配符，比如\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155\\n\\n准备数据集。数据集的格式有要求， 必须有label列。 必须有train、valid 2个数据集。且只能有这2个 必须放到同一个父目录下。 目前我们选择的机型有限， parquet格式的训练样本最好不要超过60G ，否则可能出现内存被打爆无法训练的情况。 train数据必须分片（如果使用AiLab的api提交，可以自动帮用户分片）。最终的目录形式如下：即一个repartition的目录，这是训练集，一个valid.parquet文件，这是验证集。用来辅助early stop 准备训练参数。训练参数是通过一个yaml文件定义的。示例如下： yml 注意，\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155\\n\\n分布式训练时，平台会自动把训练集和测试集的数据平均分配到各个ray的worker上，大大减少了load data带来的开销，同时减少了单节点的内存压力，这样总内存的使用量不变，但是我们可以使用多个节点增加cpu数量，提高训练效率。比如我们可以申请10台32核，64GB的高性能机器，实现320核+640GB的训练效果。这个是单机提供不了的。 如何提交任务 前提条件 提交任务之前，我们需要具备2个条件 准备数据集。数据集的格式有要求， 必须有label列。 必须有train、valid 2个数据集。且只能有这2个 必须放到同一个父目录下。 目前我们选择的机型有限，\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155\\n\\nLightGBM分布式训练（CPU、GPU） 基于Ray框架 原理 Ray是一款非常通用的机器学习框架，同时支持LightGBM，XGboost，pytorch等多种模型框架的cpu，gpu及分布式训练。 它的LightGBM分布式训练方法本身也是利用了LightGBM自身的分布式训练功能，Ray只是为其提供了一套多机的分布式计算框架。关于LightGBM的分布式训练原理可以参考其官方文档： https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-distributed-learning\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155\\n\\n即使分布式能发挥一定提速效果，但训练轮数增加则总时长就差不多了。收益就很小了。 Ray 官网： https://www.ray.io/ 主要目的是构建一个通用的分布式的模型训练、推理、调参框架。支持数据处理，但是这些数据处理主要是服务于模型训练相关，不太适合进行通用的大数据处理任务。 而且，它在数据处理，模型训练、调参等很多方面进行了重新设计，有一定的学习成本。 它也支持分布式LightGBM训练，并参考dask的源代码，在它基础上支持了early_stop。 所以，后面我们就采用了Ray作为分布式训练框架。 系统结构 我们的系统主要分如下几个部分（以Ray为例）: Automl\\xa0→ Pai\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570\\n\\n模型训练是通过多个pod进行并行计算的。Optuna根据当前已经得到的metric，选择下一组较优的参数进行新的尝试。直到总尝试次数达到某个设置的值，则结束。 使用手册 镜像选择 目前必须选择固定镜像，且仅支持cpu版本 yqg-harbor-registry-vpc.cn-beijing.cr.aliyuncs.com/risk-ai/pai/optuna:v1.0 模版选择 目前我们已经创建了一个模版 firstloan_api_lgb_hpo，可以在 https://automl.fintopia.tech/ai-resource/container 页面找到。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=85650497\\n\\n--input_feature_dir 支持目录或者文件，feather或者parquet格式。为了join label和feature，需要保证名为key_id的列在2份数据中都存在\\n  --output_dir 脚本会在下面自动创建日期子目录，日期目录下又会创建dataset/和model/两个子目录，分别存放抽取的特征和训练好的模型\\n  --run_mode run mode, 0: both feature sampling and training, 1: train only, 2: feature only\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=80898239，问题是：.. 参数建议如何设置\\nAI: 根据context文本，我认为答案是：'},\n",
       " {'role': 'assistant',\n",
       "  'content': '参数建议可以参考以下几个方面进行设置：\\n1. 训练参数是通过一个yaml文件定义的，你需要根据具体的训练需求来设置这些参数。具体可以参考给出的yml示例。source: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155\\n2. 数据集需要满足特定的格式要求，包括必须有label列，必须有train、valid两个数据集，且只能有这两个，必须放到同一个父目录下。source: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155\\n3. 如果是使用分布式训练，数据集需要被分片，且分片个数等于并行pod数的整数倍，同时数据集必须是parquet格式。source: https://wiki.fintopia.tech/pages/viewpage.action?pageId=105312091\\n\\n总的来说，具体的参数设置需要根据你的具体任务需求和数据集特性来决定。'}]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'打开AILab一直停留在黑屏页面，大部分情况下，用户打开AILab应该可以直接进入Notebook页面。不过偶尔会卡在上述页面，此时点击一下右上角的关闭按钮即可。有的时候是因为登录过期了，这个字显示的非常不明显，需要自己看看。\\nsource: https://wiki.fintopia.tech/display/riskDev/0.+FAQ'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"ailab实例一直黑屏\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['如何进行分布式LightGBM模型训练',\n",
       " '进行分布式LightGBM模型训练，可以基于Ray框架来实现。Ray是一款支持LightGBM等多种模型框架的分布式训练机器学习框架。它利用LightGBM自身的分布式训练功能，为LightGBM提供了一套多机的分布式计算框架。在分布式训练时，Ray会调用LightGBM官方提供的训练API进行机器组网，并开启分布式训练。此外，为了提高GPU利用率和训练效率，可以在分布式模式下采用特定的训练算法，如voting。但需要注意，分布式训练可能会导致一定的精度损失。更多细节和原理可以参考LightGBM的官方文档：https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-distributed-learning 。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=97186155、https://wiki.fintopia.tech/pages/viewpage.action?pageId=106844570',\n",
       " '需要具备哪些前提条件',\n",
       " '为了使用自研的分布式LightGBM打分模版，需要具备以下两个前提条件：1是数据集必须是分片好的，而且分片个数等于并行pod数的整数倍；2是数据集必须是parquet格式。详情请参考：https://wiki.fintopia.tech/pages/viewpage.action?pageId=105312091',\n",
       " '参数建议如何设置',\n",
       " '参数设置需要根据具体的模型训练任务和数据集特性来决定。在LightGBM的分布式训练中，可以考虑调整诸如\"bagging_freq\", \"max_bin\", \"min_data_in_bin\", \"min_data_in_leaf\", \"min_sum_hessian_in_leaf\", \"min_child_weight\", \"boosting_type\", \"objective\", \"metric\"等参数。具体的参数值需要通过实验和调优来确定，可以使用Optuna等调参工具来帮助选择较优的参数组合。并没有固定的参数设置建议，因为每个数据集和任务都是独特的。\\nsource: https://wiki.fintopia.tech/pages/viewpage.action?pageId=85651968、https://wiki.fintopia.tech/pages/viewpage.action?pageId=8565049']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_ans = [c['content'] for c in llm.conversation_history if c['role'] == 'assistant']\n",
    "user_quest = llm.user_question_history\n",
    "qa_pairs = []\n",
    "for q, a in zip(user_quest, ai_ans):\n",
    "    qa_pairs += [q, a]\n",
    "qa_pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-wechat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
